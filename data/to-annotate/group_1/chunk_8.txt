a restricted dataset. [93] A large-scale convolutional-RNN-CTC architecture was presented
in 2018 by Google DeepMind, achieving 6 times better performance than human experts.
[94] In 2019, Nvidia launched two CNN-CTC ASR models, Jasper and QuarzNet, with an
overall performance word error rate (WER) of 3%. [95][96] Similar to other deep learning
applications, transfer learning and domain adaptation are important strategies for
reusing and extending the capabilities of deep learning models, particularly due to
the small size of available corpora in many languages and/or specific domains. [97][98][99]
In 2018, researchers at MIT Media Lab announced preliminary work on AlterEgo, a device
that uses electrodes to read the neuromuscular signals users make as they subvocalize.
[100] They trained a convolutional neural network to translate the electrode signals
into words. [101] Attention-based ASR models were introduced by Chan et al. of Carnegie
Mellon University and Google Brain, and Bahdanau et al. of the University of Montreal
in 2016. [102][103] The model named "Listen, Attend and Spell" (LAS), literally "listens"
to the acoustic signal, pays "attention" to all parts of the signal and "spells" out
the transcript one character at a time. Unlike CTC-based models, attention-based models
require conditional-independence assumptions and can learn all the components of a
speech recognizer directly. This means that during deployment, no a priori language
model is required, making it less demanding for applications with limited memory.
Attention-based models immediately outperformed CTC models (with or without an external
language model) and continued improving. [104] Latent Sequence Decomposition (LSD)
was proposed by Carnegie Mellon University, MIT, and Google Brain to directly emit
sub-word units that are more natural than English characters. [105] The University
of Oxford and Google DeepMind extended LAS to "Watch, Listen, Attend and Spell" (WLAS)
to handle lip reading and surpassed human-level performance. [106] Voice commands
may be used to initiate phone calls, select radio stations, or play music. Voice recognition
capabilities vary across car make and model. Some models offer natural-language speech
recognition, allowing the driver to use full sentences and common phrases in a conversational
style. With such systems, fixed commands are not required. [citation needed] Automatic
pronunciation assessment is the use of speech recognition to verify the correctness
of speech, [107] as distinguished from assessment by a person. [108] Also called speech
verification, pronunciation evaluation, and pronunciation scoring, the main application
of this technology is computer-aided pronunciation teaching (CAPT) when combined with
computer-aided instruction for computer-assisted language learning (CALL), speech
remediation, or accent reduction. Pronunciation assessment does not determine unknown
speech (as in dictation or automatic transcription) but instead, compares speech to
a reference model for the words spoken, [109][110] sometimes with inconsequential
prosody such as intonation, pitch, tempo, rhythm, and stress. [111] Pronunciation
assessment is also used in reading tutoring, for example in products such as Microsoft
Teams[112] and Amira Learning. [113] Pronunciation assessment can also be used to
help diagnose and treat speech disorders such as apraxia. [114] Assessing intelligibility
is essential for avoiding inaccuracies from accent bias, especially in high-stakes