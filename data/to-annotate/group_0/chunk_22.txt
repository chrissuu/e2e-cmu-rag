2021, Twitter began beta testing a feature called Safety Mode. [362] The functionality
aims to limit unwelcome interactions through automated detection of negative engagements.
If a user has Safety Mode enabled, authors of tweets that are identified by Twitter's
technology as being harmful or exercising uninvited behavior will be temporarily unable
to follow the account, send direct messages, or see tweets from the user with the
enabled functionality during the temporary block period. [363] Jarrod Doherty, senior
product manager at Twitter, stated that the technology in place within Safety Mode
assesses existing relationships to prevent blocking accounts that the user frequently
interacts with. [362] In January 2016, Twitter was sued by the widow of a U. S. man
killed in the 2015 Amman shooting attack, claiming that allowing the Islamic State
of Iraq and the Levant (ISIL) to continually use the platform, including direct messages
in particular, [364] constituted the provision of material support to a terrorist
organization, which is illegal under U. S. federal law. Twitter disputed the claim,
stating that "violent threats and the promotion of terrorism deserve no place on Twitter
and, like other social networks, our rules make that clear". [365][366] The lawsuit
was dismissed by the United States District Court for the Northern District of California,
upholding the Section 230 safe harbor, which dictates that the operators of an interactive
computer service are not liable for the content published by its users. [366][367]
The lawsuit was revised in August 2016, providing comparisons to other telecommunications
devices. [364] The second amended complaint was dismissed by the district court, a
decision affirmed on appeal to the U. S. Court of Appeals for the Ninth Circuit on
January 31, 2018. [368] Twitter suspended multiple parody accounts that satirized
Russian politics in May 2016, sparking protests and raising questions about where
the company stands on freedom of speech. [369] Following public outcry, Twitter restored
the accounts the next day without explaining why the accounts had been suspended.
[370] The same day, Twitter, along with Facebook, Google, and Microsoft, jointly agreed
to a European Union code of conduct obligating them to review "[the] majority of valid
notifications for removal of illegal hate speech" posted on their services within
24 hours. [371] In August 2016, Twitter stated that it had banned 235, 000 accounts
over the past six months, bringing the overall number of suspended accounts to 360,
000 accounts in the past year, for violating policies banning use of the platform
to promote extremism. [372] On May 10, 2019, Twitter announced that they suspended
166, 513 accounts for promoting terrorism in the July–December 2018 period, saying
there was a steady decrease in terrorist groups trying to use the platform owing to
its "zero-tolerance policy enforcement". According to Vijaya Gadde, Legal, Policy
and Trust and Safety Lead at Twitter, there was a reduction of 19% terror related
tweets from the previous reporting period (January–June 2018). [373][374][375][376][377]