analysis of data in general. Today, statistics is widely employed in government, business,
and natural and social sciences. The mathematical foundations of statistics developed
from discussions concerning games of chance among mathematicians such as Gerolamo
Cardano, Blaise Pascal, Pierre de Fermat, and Christiaan Huygens. Although the idea
of probability was already examined in ancient and medieval law and philosophy (such
as the work of Juan Caramuel), probability theory as a mathematical discipline only
took shape at the very end of the 17th century, particularly in Jacob Bernoulli's
posthumous work Ars Conjectandi. [40] This was the first book where the realm of games
of chance and the realm of the probable (which concerned opinion, evidence, and argument)
were combined and submitted to mathematical analysis. [41] The method of least squares
was first described by Adrien-Marie Legendre in 1805, though Carl Friedrich Gauss
presumably made use of it a decade earlier in 1795. [42] The modern field of statistics
emerged in the late 19th and early 20th century in three stages. [43] The first wave,
at the turn of the century, was led by the work of Francis Galton and Karl Pearson,
who transformed statistics into a rigorous mathematical discipline used for analysis,
not just in science, but in industry and politics as well. Galton's contributions
included introducing the concepts of standard deviation, correlation, regression analysis
and the application of these methods to the study of the variety of human characteristicsâ€”height,
weight and eyelash length among others. [44] Pearson developed the Pearson product-moment
correlation coefficient, defined as a product-moment, [45] the method of moments for
the fitting of distributions to samples and the Pearson distribution, among many other
things. [46] Galton and Pearson founded Biometrika as the first journal of mathematical
statistics and biostatistics (then called biometry), and the latter founded the world's
first university statistics department at University College London. [47] The second
wave of the 1910s and 20s was initiated by William Sealy Gosset, and reached its culmination
in the insights of Ronald Fisher, who wrote the textbooks that were to define the
academic discipline in universities around the world. Fisher's most important publications
were his 1918 seminal paper The Correlation between Relatives on the Supposition of
Mendelian Inheritance (which was the first to use the statistical term, variance),
his classic 1925 work Statistical Methods for Research Workers and his 1935 The Design
of Experiments, [48][49][50] where he developed rigorous design of experiments models.
He originated the concepts of sufficiency, ancillary statistics, Fisher's linear discriminator
and Fisher information. [51] He also coined the term null hypothesis during the Lady
tasting tea experiment, which "is never proved or established, but is possibly disproved,
in the course of experimentation". [52][53] In his 1930 book The Genetical Theory
of Natural Selection, he applied statistics to various biological concepts such as
Fisher's principle[54] (which A. W. F. Edwards called "probably the most celebrated
argument in evolutionary biology") and Fisherian runaway, [55][56][57][58][59][60]