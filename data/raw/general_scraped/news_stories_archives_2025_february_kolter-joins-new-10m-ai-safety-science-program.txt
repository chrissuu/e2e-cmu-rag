Zico Kolter (opens in new window) will tackle critical artificial intelligence safety issues as part of the new AI Safety Science program (opens in new window) established by former Google CEO Eric Schmidt.
Kolter, head of the Machine Learning Department (opens in new window) at Carnegie Mellon University's School of Computer Science, will explore the causes of adversarial transfer — a phenomenon where attacks developed for one AI model are also effective when applied to other models.
Kolter's work has long focused on the safety and robustness of AI systems. Research published in 2023 (opens in new window) showed how to circumvent AI safety guardrails built into popular systems like ChatGPT, Claude and Google Bard. Kolter is now on the board of OpenAI, the developers of ChatGPT.
"We're thrilled to work together with Schmidt Sciences to conduct research on the fundamental scientific challenges underlying AI safety and robustness," Kolter said.
Schmidt Sciences, founded by Schmidt and his wife, Wendy, started AI Safety Science to make safety science an integral part of AI innovation and provide $10 million to support foundational research. The program will foster a collaborative global research community and offer computational support from the Center for AI Safety (opens in new window) and API access from OpenAI (opens in new window) .
The AI Safety Science program selected 27 projects aimed at developing the fundamental science critical to understanding the safety properties of AI systems, calling the area of study essential yet underfunded. The initial projects seek to develop well-founded, concrete, implementable technical methods for testing and evaluating large language models (LLMs) so they are less likely to cause harm, make errors or be misused.
"As AI systems advance, we face the risk that they will act in ways that contradict human values and interests — but this risk is not inevitable," Eric Schmidt said. "With efforts like the AI Safety Science program, we can help build a future in which AI benefits us all while maintaining safeguards that protect us from harm."
Later this year, the AI Safety Science program will convene its awardees in California, where they will share their work with each other and with organizations interested in AI safety. The program also plans additional calls for proposals to bring new awardees into the program. Learn more about the program and the awardees on the AI Safety Science website (opens in new window) .