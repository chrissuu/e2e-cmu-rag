Not long into "Rogue One: A Star Wars Story," the Death Star appears through the viewport of an Imperial Star Destroyer's bridge. Reflected in that viewport is the instantly recognizable face of Grand Moff Tarkin, an Imperial baddie made famous in 1977's "Star Wars: A New Hope" and portrayed with expert villainy by Peter Cushing.
The character slowly turns, and Cushing's face fills the screen with evil cunning.
A narrow chin and clenched jaw. Prominent cheekbones jutting below cold, hard eyes. His bearing rigid, brow furrowed. His face alone says that he will suffer no fools, let alone think twice before destroying his enemies.
But how did Cushing, who died in 1994, star in 2016’s "Rogue One"? What was this magic?
That magic was Carnegie Mellon University alumnus Kiran Bhat, who earned his Ph.D. in robotics from the School of Computer Science in 2004 and has spent his career using computer vision technologies to make animated characters look more lifelike, first at Lucasfilm's Industrial Light and Magic (ILM) and then through his own startup, Loom.ai.
It's tempting to assume that someone who began his career at ILM came to computer vision and graphics through a love of animation. But that's not the case with Kiran, whose first love was robots.
"Robotics started this whole journey," Kiran says. "What sparked my interest in this field is actually trying to figure out how humans and nature move and interact so gracefully in their environments, whereas any man-made systems — even sophisticated systems in the late 1990s — were clunky."
That desire to give robots more lifelike motion inspired Kiran to apply to CMU's graduate program in robotics after he completed his bachelor's degree in India. Once in Pittsburgh, he began studying locomotion in robotics with longtime faculty member Pradeep Khosla — now chancellor of the University of California San Diego.
But Kiran kept coming back to nature, wondering how to observe something in the world, capture the dynamics of its actions, and map those actions to a computer system. Because resources for studying the physical interactions of robots were still limited, Kiran switched to simulations. Computer vision became his passion.
"I realized the right thing to focus on was a controlled, yet rich, simulated environment, and to focus on how to capture an object's parameters and movements by observing the real world," he says.
Kiran gives the example of a juggler.
"If you can point your camera at a juggler, you see the complex dynamics of these objects floating around in the air," he says.
His challenge was automatically determining the parameters of that action through video and creating a realistic simulation.
The path from that research to ILM was simple. ILM had been making movies that relied on puppets (think: Yoda) interacting with humans on screen. But in the early 2000s, they made the decision to go fully digital.
"There was a huge challenge in the industry to figure out how to build systems for rendering and animating characters that felt lifelike, especially when you were putting them beside a human in those shots," Kiran says.
He took an internship at ILM, where he applied a computer vision technique known as structure from motion to solve for the motion and lens parameters from principal camera footage. (A technique, by the way, that CMU pioneered in the 1990s.)
During that internship, he interacted with a broad range of ILM engineers and artists and learned about the key challenges the industry faced in bringing these animations to life. Specifically, he observed the painstaking process of making Yoda's cape look lifelike, especially in scenes where he interacted with the human actor playing Obi-Wan Kenobi. Engineers would set up simulations for the item in question (e.g., the cape), and artists would spend hours fine-tuning their parameters until the animation looked as realistic as possible. They then repeated the arduous process each time the director wanted a change.
Kiran envisioned a world where computer vision made simulations look realistic with less effort and spent his Ph.D. working to make it happen.
After graduation, Kiran returned to ILM full time. Alongside his colleagues, he iterated and fine-tuned a computer vision technology where a camera captured a real-world object and created a digital duplicate that artists could easily edit. Actors — Mark Ruffalo as the Hulk is an excellent example — would wear camera-outfitted helmets that recorded their facial expressions, mannerisms and movement. This recording became a digital photocopy that artists and animators could easily tweak to create whatever emotion or action the director wanted to portray to "make the green guy look like the human," Kiran says of the Hulk.
Which leads back to Peter Cushing.