The technological development of the US iron and steel industry has closely mirrored that of other countries. In the 1800s, the US switched from charcoal to coal in ore smelting, adopted the Bessemer process, and saw the rise of very large integrated steel mills. In the 20th century, the US industry transitioned from the open hearth furnace to the basic oxygen steelmaking process. After peaking in the 1940s and 1950s, the US iron and steel industry shifted toward smaller mini-mills and specialty mills that use iron and steel scrap instead of iron ore.

Before the 19th century, iron production relied on charcoal. However, Britain’s once-abundant forests could no longer meet the nation’s growing demand for iron.

By 1700, Britain was becoming increasingly dependent on iron imported from its sometimes-adversary Sweden. Britain looked to the seemingly limitless forests of its American colonies to supply Britain with iron. British investors started an iron furnace near Perryville, Maryland, which in 1718 started exporting iron back to Britain. That success prompted formation of more companies, which built numerous iron furnaces around Chesapeake Bay, supplied by bog iron ore, which was widespread. By 1751, Virginia and Maryland were exporting 2,950 tons of pig iron to Britain each year; at the time, British iron production was about 20,000 tons per year.[1]

While the Chesapeake Bay furnaces were established for export, iron furnaces were established in the 1700s throughout the American colonies for domestic consumption. Iron furnaces were located along rivers to supply water power. Also required were forests for charcoal, iron ore, and limestone for flux. In addition, the furnace needed to be close to a major market or close to water transport.

British business interests were split on colonial iron: manufacturers appreciated the lower prices due to colonial imports, but the British iron and steel industry objected to the competition. Parliament compromised in the Iron Act 1750, which eliminated the import duty on colonial pig iron, but barred the manufacture of steel or of iron plate in the colonies. Despite the Iron Act 1750, colonial governments continued to develop their iron industries.

By 1776, up to 80 iron furnaces throughout the American colonies were producing about as much iron as Britain itself. If one estimate of 30,000 tons of iron each year is accurate, then the newly formed United States was the world's third-largest iron producer, after Sweden and Russia.

Notable pre-19th-century iron furnaces in the US

Because wood for charcoal was available throughout the eastern states, iron smelters were located close to iron ore. Although the bog iron ores mined in colonial days were widespread, the deposits were also small, and quickly exhausted. In the late 1700s the iron furnaces moved away from the bog iron ore of the coastal swamps, to larger iron ore deposits further inland. Inland locations also allowed the furnaces to be closer to sources of limestone, which was used as a flux in iron smelting. The proximity to larger ore deposits favored larger, more permanent iron smelters.[2]

Most US iron smelting before 1850 took place near iron deposits in eastern Pennsylvania, New York, and northern New Jersey.[3] New Jersey's principal iron ore district, at Dover, supported iron smelters beginning in 1710. The Cornwall Iron Furnace in Pennsylvania was established next to an iron deposit. The Adirondack iron ore district of New York also supported iron smelters.[4]

The movement away from charcoal in US iron smelting began in 1827, when a puddling furnace in Phoenixville, Pennsylvania started using anthracite coal. Blast furnaces continued to use only charcoal until about 1840, when coke from coal started replacing charcoal as the fuel and reducing agent.[5] Coke has a higher crushing strength than charcoal, allowing larger smelting furnaces.[6] Because iron and steel-making at the time consumed more coal than iron ore, the steel mills moved closer to the coal mines to minimize transportation costs. A problem of coke was that it carried impurities such as sulfur, which degraded the quality of the steel. Although coke quickly became the dominant fuel for iron-smelting, in 1884 charcoal was still used to make ten percent of iron and steel in the US. The use of charcoal for steelmaking survived in the US on a small scale until 1945.

The Lackawanna Valley in Pennsylvania was rich in anthracite coal and iron deposits. Brothers George W. Scranton and Seldon T. Scranton moved to the valley in 1840 and settled in the five-house town of Slocum's Hollow (now Scranton) to establish an iron forge.[7][8] The most common processes for creating blister steel and crucible steel were slow and extremely expensive. The Scrantons instead used the new "hot blast method," developed in Scotland in 1828.[7] The hot blast method solved the problem of impurities from the coke, by burning them off. The Scrantons also experimented with anthracite to make steel, rather than charcoal or bituminous coal.[9]

The replacement of charcoal with coal in the steel-making process revolutionized the industry, and tied steelmaking to coal-mining areas. In the 1800s, making a ton of steel required a greater weight of coal than iron ore.  Therefore, it was more economical to locate closer to the coal mines. Pittsburgh, Pennsylvania, surrounded by large coal deposits and at the junction of three navigable rivers, was an ideal location for steelmaking.

Notable early 19th-century iron furnaces in the US

In 1856, Englishman Henry Bessemer invented the Bessemer process, which allowed for mass production of steel from molten pig iron, reducing the cost of making steel by more than 50%. The first American steel mill to use the process was constructed in 1865 in Troy, New York. In 1875, the largest-yet steel mill, Edgar Thomson Steel Works in the Pittsburgh area, was built to use the Bessemer process, financed by industrialist Andrew Carnegie.

The tremendous iron ore deposits around Lake Superior were located far from coal deposits, and so were shipped to ports on the southern Great Lakes that were closer to the coal mines of Pennsylvania, Ohio, Indiana, and Illinois. Large integrated steel mills were built in Chicago, Detroit, Gary, Indiana, Cleveland, and Buffalo, New York, to handle the Lake Superior ore.

Cleveland's first blast furnace was built in 1859. In 1860, the steel mill employed 374 workers. By 1880, Cleveland was a major steel producer, with ten steel mills and 3,000 steelworkers.[10]

The city of Gary, Indiana was founded in 1906 by United States Steel Corporation to serve the Gary Works.

The Lackawanna Steel Company built a large integrated steel works near Buffalo, which began producing steel from Lake Superior ore in 1903. The company had made steel in Scranton, Pennsylvania since 1840, but moved to provide easier access to iron ore, and in an unsuccessful attempt to avoid labor troubles.

Birmingham, Alabama became a major steel producer in the late 1800s, using locally mined coal and iron ore. The iron ore was mined from the Red Mountain Formation of Silurian age.

Notable defunct late 19th-century iron and steel furnaces in the US

As the only major steel maker not harmed during World War II, the United States iron and steel industry reached its maximum world importance during and just after World War II. In 1945, the US produced 67% of the world's pig iron, and 72% of the steel. By comparison, 2014 percentages were 2.4% of the pig iron, and 5.3% of the steel production.

Although US iron and steel output continued to grow overall through the 1950s and 1960s, the world steel industry grew much faster, and the US share of world production shrank. In the 1960s, the US became a major importer of steel, mostly from Japan.

US production of iron and steel peaked in 1973, when the US industry produced a combined total of 229 million metric tons of iron and steel. But US iron and steel production dropped drastically during the recession of the late 1970s and early 1980s. From a combined iron and steel production of 203 million tons in 1979, US output fell almost in half, to 107 million tons in 1982. Some steel companies declared bankruptcy, and many permanently closed steelmaking plants. By 1989, US combined iron and steel production recovered to 142 million tons, a much lower level than in the 1960s and 1970s.

The causes of the sudden decline are disputed. Among the many causes alleged have been: dumping of foreign imports below cost, high labor costs, poor management, unfavorable tax policies, and costs of environmental controls. One recent study found that hypotheses based on declining domestic production (such as foreign imports displacing domestic product) cannot explain the magnitude of the decline in employment, as shipments of domestic steel products in 2005 were similar to the level of the early 1960s, whereas employment in the domestic sector had plummeted by approximately 75% over the same time period. Instead, the majority of the losses could be accounted for by rising productivity, principally through technological efficiencies and the shift from traditional steel plants to mini mills.[11]

Cleveland had nine integrated steel mills in the 1970s. US Steel closed one mill in 1979, then shut down its six remaining Cleveland mills in 1984. Cleveland's two remaining steel producers, Republic Steel and Jones & Laughlin, merged to form LTV Steel in June 1984. LTV Steel declared bankruptcy in 2000. The operation, on both sides of the Cuyahoga River, is Cleveland's last remaining integrated steel mill, now owned by Cleveland Cliffs.

In the Pittsburgh region, mill closures led to a regional unemployment rate that peaked at 17.1% in January 1983, with local unemployment rates as high as 27.1% in Beaver County.[12] Between 1970 and 1990, the region lost 30% of its population.[12]

The number of integrated steel mills has continued to decline, and in 2014, only 11 integrated mills were operating in the US. Most of the steel produced has been by the growing number of mini-mills, also called specialty mills, which in 2014 numbered 113. In 1981, mini-mills produced an estimated 15% of US steel.[13] Since 2002, steel produced by electric arc furnace, the process used by the mini-mills, has produced more than half the steel made in the US. Many companies operating integrated mills also have mini-mills.

A number of bankruptcies and acquisitions between 2000 and 2014 reversed the trend of industry fragmentation. In 2000, the top three steelmakers (Nucor, US Steel, and Bethlehem Steel) had 28% of the steelmaking capacity, and the top ten had 58%. By 2014, the top three (Nucor, ArcelorMittal, and US Steel) accounted for 56% of the steel capacity, and the top ten 87%.[14]

Notable defunct 20th-century steel furnaces in the US
